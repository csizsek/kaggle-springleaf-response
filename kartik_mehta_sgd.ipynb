{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "from random import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train='input/train.csv'\n",
    "test='input/test.csv'\n",
    "submission = 'output/sgd_subm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = .005  # learning rate\n",
    "beta = 1\n",
    "L1 = 0.       # L1 regularization, larger value means more regularized\n",
    "L2 = 0.       # L2 regularization, larger value means more regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 2 ** 24             # number of weights to use\n",
    "interaction = True     # whether to enable poly2 feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 1          # learn training data for N passes\n",
    "holdafter = None   # data after date N (exclusive) are used as validation\n",
    "holdout = 100      # use every N training instance for holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# class, function, generator definitions #####################################\n",
    "##############################################################################\n",
    "\n",
    "class gradient_descent(object):\n",
    "\n",
    "    def __init__(self, alpha, beta, L1, L2, D, interaction):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "        # feature related parameters\n",
    "        self.D = D\n",
    "        self.interaction = interaction\n",
    "\n",
    "        # model\n",
    "        # G: squared sum of past gradients\n",
    "        # w: weights\n",
    "        self.w = [0.] * D  \n",
    "        self.G = [0.] * D \n",
    "\n",
    "    def _indices(self, x):\n",
    "        ''' A helper generator that yields the indices in x\n",
    "\n",
    "            The purpose of this generator is to make the following\n",
    "            code a bit cleaner when doing feature interaction.\n",
    "        '''\n",
    "\n",
    "        # first yield index of the bias term\n",
    "        yield 0\n",
    "\n",
    "        # then yield the normal indices\n",
    "        for index in x:\n",
    "            yield index\n",
    "\n",
    "        # now yield interactions (if applicable)\n",
    "        if self.interaction:\n",
    "            D = self.D\n",
    "            L = len(x)\n",
    "\n",
    "            x = sorted(x)\n",
    "            for i in xrange(L):\n",
    "                for j in xrange(i+1, L):\n",
    "                    # one-hot encode interactions with hash trick\n",
    "                    yield abs(hash(str(x[i]) + '_' + str(x[j]))) % D\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' Get probability estimation on x\n",
    "\n",
    "            INPUT:\n",
    "                x: features\n",
    "\n",
    "            OUTPUT:\n",
    "                probability of p(y = 1 | x; w)\n",
    "        '''\n",
    "\n",
    "        # model\n",
    "        w = self.w\t\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = 0.\n",
    "        for i in self._indices(x):\n",
    "            wTx += w[i]\n",
    "\n",
    "        # bounded sigmoid function, this is the probability estimation\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        ''' Update model using x, p, y\n",
    "\n",
    "            INPUT:\n",
    "                x: feature, a list of indices\n",
    "                p: click probability prediction of our model\n",
    "                y: answer\n",
    "\n",
    "            MODIFIES:\n",
    "                self.G: increase by squared gradient\n",
    "                self.w: weights\n",
    "        '''\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "\n",
    "        # model\n",
    "        w = self.w\n",
    "        G = self.G\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "        # update z and n\n",
    "        for i in self._indices(x):\n",
    "            G[i] += g*g\n",
    "#            w[i] -= alpha*1/sqrt(n[i]) * (g - L2*w[i]) ## Learning rate reducing as 1/sqrt(n_i) : ALso gives good performance but below code gives better results\n",
    "            w[i] -= alpha/(beta+sqrt(G[i])) * (g - L2*w[i]) ## Learning rate reducing as alpha/(beta + sqrt of sum of g_i)\n",
    "\n",
    "        self.w = w\n",
    "        self.G = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(p, y):\n",
    "    ''' FUNCTION: Bounded logloss\n",
    "\n",
    "        INPUT:\n",
    "            p: our prediction\n",
    "            y: real answer\n",
    "\n",
    "        OUTPUT:\n",
    "            logarithmic loss of p given y\n",
    "    '''\n",
    "\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(path, D):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: a list of hashed and one-hot-encoded 'indices'\n",
    "               we only need the index since all values are either 0 or 1\n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "\n",
    "    for t, row in enumerate(DictReader(open(path), delimiter=',')):\n",
    "      \n",
    "        try:\n",
    "            ID=row['ID']\n",
    "            del row['ID']\n",
    "        except:\n",
    "            pass\n",
    "        # process clicks\n",
    "        y = 0.\n",
    "        target='target'#'IsClick' \n",
    "        if target in row:\n",
    "            if row[target] == '1':\n",
    "                y = 1.\n",
    "            del row[target]\n",
    "\n",
    "        # extract date\n",
    "\n",
    "        # turn hour really into hour, it was originally YYMMDDHH\n",
    "\n",
    "        # build x\n",
    "        x = []\n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "\n",
    "            # one-hot encode everything with hash trick\n",
    "            index = abs(hash(key + '_' + value)) % D\n",
    "            x.append(index)\n",
    "\n",
    "        yield ID,  x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# start training #############################################################\n",
    "##############################################################################\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# initialize ourselves a learner\n",
    "learner = gradient_descent(alpha, beta, L1, L2, D, interaction)\n",
    "\n",
    "# start training\n",
    "print('Training Learning started; total 150k training samples')\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    count = 0\n",
    "    for t,  x, y in data(train, D):  # data is a generator\n",
    "\n",
    "        p = learner.predict(x)\n",
    "        loss += logloss(p, y)\n",
    "        learner.update(x, p, y)\n",
    "        count+=1\n",
    "        if count%15000==0:\n",
    "            print('%s\\tencountered: %d\\tcurrent logloss: %f' % (datetime.now(), count, loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# start testing, and build Kaggle's submission file ##########################\n",
    "##############################################################################\n",
    "count=0\n",
    "print('Testing started; total 150k test samples')\n",
    "with open(submission, 'w') as outfile:\n",
    "    outfile.write('ID,target\\n')\n",
    "    for  ID, x, y in data(test, D):\n",
    "        count+=1\n",
    "        if count%15000==0:\n",
    "            print('%s\\tencountered: %d' % (datetime.now(), count))\n",
    "        p = learner.predict(x)\n",
    "        outfile.write('%s,%s\\n' % (ID, str(p)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
